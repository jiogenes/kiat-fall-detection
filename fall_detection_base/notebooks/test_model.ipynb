{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea458b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, einops, pathlib, cv2, json\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import patches\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "\n",
    "def preprocess_keypoints(keypoints, skeletons):\n",
    "    invalid_keypoint_index = keypoints.sum(axis=-1) == 0\n",
    "    invalid_vector_index = []\n",
    "\n",
    "    for i in range(len(skeletons)):\n",
    "        s, e = skeletons[i]\n",
    "        if s in invalid_keypoint_index or e in invalid_keypoint_index:\n",
    "            invalid_vector_index.append(i)\n",
    "\n",
    "    vec = keypoints[skeletons[:, 1]] - keypoints[skeletons[:, 0]]\n",
    "    vec /= (np.linalg.norm(vec, ord=2, axis=-1, keepdims=True) + 1e-5)\n",
    "    vec[invalid_vector_index] = 0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def preprocess_keypoints_batch(keypoints, skeletons):\n",
    "    vecs = np.stack([preprocess_keypoints(kpts, sks) for kpts, sks in zip(keypoints, skeletons)])\n",
    "    return vecs\n",
    "\n",
    "\n",
    "class FallDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim=32):\n",
    "        super(FallDetector, self).__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(nn.Linear(19*3, feature_dim), nn.LeakyReLU(0.1), nn.Linear(feature_dim, feature_dim))\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        self.detector = nn.LSTM(feature_dim, feature_dim, 1, batch_first=True,)\n",
    "        self.classifier = nn.Sequential(nn.Linear(feature_dim, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, frames):\n",
    "        frames = frames.flatten(-2, -1)\n",
    "        x = self.feature_extractor(frames)\n",
    "        x, (hn, cn) = self.detector(x)\n",
    "        x = self.classifier(x[:, -1]).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1ae72568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FallDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "701856d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 15, 17*2)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45251e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_list = self._load_data_list()\n",
    "\n",
    "    def _load_data_list(self):\n",
    "        data_list = []\n",
    "        for path in pathlib.Path(self.data_dir).glob(\"keypoints_*.json\"):\n",
    "            data_list.append((path, int(\"fall\" in path.name)))\n",
    "\n",
    "        return data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, fallen = self.data_list[index]\n",
    "        frames = self.read_keypoints(path)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32)\n",
    "        fallen = torch.tensor(fallen, dtype=torch.long)\n",
    "        return frames, fallen\n",
    "    \n",
    "    def read_keypoints(self, path):\n",
    "        with open(str(path)) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        keypoints = np.array([kpts[0] for kpts in data[\"keypoints\"] if len(kpts) > 0])\n",
    "        keypoints += np.random.normal(loc=0, scale=(keypoints.max() - keypoints.min())/100, size=keypoints.shape)\n",
    "        skeleton = np.array([sks[0] for sks in data[\"skeleton\"] if len(sks) > 0]) - 1\n",
    "        # print(keypoints.shape, skeleton.shape)\n",
    "        keypoints = preprocess_keypoints_batch(keypoints, skeleton)\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b78f8997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = URDataset(\"../output/UR_fall_detection2\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06fa4783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.09315490722656 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([203, 19, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = dataset[20][0]\n",
    "frames.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9c65d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/UR_fall_detection2/keypoints_parsed_adl-10-cam0.json\n",
      "0 torch.Size([299, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-22-cam0.json\n",
      "1 torch.Size([32, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-23-cam0.json\n",
      "2 torch.Size([99, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-33-cam0.json\n",
      "3 torch.Size([135, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-26-cam0.json\n",
      "4 torch.Size([95, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-37-cam0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4020468/2088773368.py:20: RuntimeWarning: invalid value encountered in divide\n",
      "  vec /= np.linalg.norm(vec, ord=2, axis=-1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([230, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-05-cam0.json\n",
      "6 torch.Size([110, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-26-cam0.json\n",
      "7 torch.Size([38, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-25-cam0.json\n",
      "8 torch.Size([110, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-32-cam0.json\n",
      "9 torch.Size([121, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-27-cam0.json\n",
      "10 torch.Size([100, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-12-cam0.json\n",
      "11 torch.Size([108, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-06-cam0.json\n",
      "12 torch.Size([83, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-01-cam0.json\n",
      "13 torch.Size([139, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-20-cam0.json\n",
      "14 torch.Size([107, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-08-cam0.json\n",
      "15 torch.Size([91, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-12-cam0.json\n",
      "16 torch.Size([203, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-16-cam0.json\n",
      "17 torch.Size([122, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-40-cam0.json\n",
      "18 torch.Size([161, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-18-cam0.json\n",
      "19 torch.Size([43, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-28-cam0.json\n",
      "20 torch.Size([53, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-22-cam0.json\n",
      "21 torch.Size([94, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-04-cam0.json\n",
      "22 torch.Size([52, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-15-cam0.json\n",
      "23 torch.Size([159, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-16-cam0.json\n",
      "24 torch.Size([55, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-11-cam0.json\n",
      "25 torch.Size([130, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-23-cam0.json\n",
      "26 torch.Size([49, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-01-cam0.json\n",
      "27 torch.Size([113, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-39-cam0.json\n",
      "28 torch.Size([135, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-13-cam0.json\n",
      "29 torch.Size([70, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-27-cam0.json\n",
      "30 torch.Size([71, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-08-cam0.json\n",
      "31 torch.Size([162, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-07-cam0.json\n",
      "32 torch.Size([180, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-05-cam0.json\n",
      "33 torch.Size([180, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-10-cam0.json\n",
      "34 torch.Size([105, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-06-cam0.json\n",
      "35 torch.Size([211, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-21-cam0.json\n",
      "36 torch.Size([124, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-14-cam0.json\n",
      "37 torch.Size([61, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-25-cam0.json\n",
      "38 torch.Size([66, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-30-cam0.json\n",
      "39 torch.Size([50, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-31-cam0.json\n",
      "40 torch.Size([132, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-17-cam0.json\n",
      "41 torch.Size([77, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-35-cam0.json\n",
      "42 torch.Size([192, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-04-cam0.json\n",
      "43 torch.Size([150, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-36-cam0.json\n",
      "44 torch.Size([210, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-28-cam0.json\n",
      "45 torch.Size([85, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-21-cam0.json\n",
      "46 torch.Size([42, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-13-cam0.json\n",
      "47 torch.Size([159, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-24-cam0.json\n",
      "48 torch.Size([37, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-30-cam0.json\n",
      "49 torch.Size([152, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-29-cam0.json\n",
      "50 torch.Size([125, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-24-cam0.json\n",
      "51 torch.Size([70, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-19-cam0.json\n",
      "52 torch.Size([109, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-03-cam0.json\n",
      "53 torch.Size([180, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-03-cam0.json\n",
      "54 torch.Size([199, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-19-cam0.json\n",
      "55 torch.Size([47, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_adl-11-cam0.json\n",
      "56 torch.Size([220, 19, 3])\n",
      "../output/UR_fall_detection2/keypoints_parsed_fall-29-cam0.json\n",
      "57 torch.Size([69, 19, 3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    frames = dataset[i][0]\n",
    "    print(i, frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6e2a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames /= frames.max(axis=1, keepdims=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38645b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(0, 10):\n",
    "    for kp in frames[i + 27]:\n",
    "        c = patches.Circle((kp[0], kp[1]), radius=0.01, fill=True, color=\"red\")\n",
    "        ax[i].add_patch(c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "221b8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FallDetector()\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f77021",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.5, 0.9))\n",
    "dataset = URDataset(\"../output/UR_fall_detection2\")\n",
    "dloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def training_step():\n",
    "    for e in range(EPOCHS):\n",
    "        accs = []\n",
    "        gts = []\n",
    "        preds = []\n",
    "        for frames, labels in dloader:\n",
    "            frames = frames.to(0)\n",
    "            labels = labels.to(0).float()\n",
    "\n",
    "            pred = model(frames)\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            cat_pred = (pred >= 0.5).float()\n",
    "            acc = (labels == cat_pred).float().mean()\n",
    "            accs.append(acc)\n",
    "            preds.append(cat_pred.detach().cpu().numpy())\n",
    "            gts.append(labels.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        gts = np.concatenate(gts)\n",
    "        preds = np.concatenate(preds)\n",
    "        f1 = f1_score(gts, preds)\n",
    "        acc = sum(accs) / len(accs)\n",
    "        print(f\"Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        if f1 > 0.9 and acc > 0.9: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea488b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.5429, F1: 0.2000\n",
      "Acc: 0.7143, F1: 0.5652\n",
      "Acc: 0.8286, F1: 0.7778\n",
      "Acc: 0.8429, F1: 0.8000\n",
      "Acc: 0.8714, F1: 0.8475\n",
      "Acc: 0.8714, F1: 0.8571\n",
      "Acc: 0.8857, F1: 0.8667\n",
      "Acc: 0.9286, F1: 0.9206\n"
     ]
    }
   ],
   "source": [
    "training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5cb842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FallDetector()\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4f16219",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.5, 0.9))\n",
    "train_dataset = URDataset(\"../output/UR_fall_detection3/train\")\n",
    "test_dataset = URDataset(\"../output/UR_fall_detection3/test\")\n",
    "train_dloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def training_step():\n",
    "    for e in range(EPOCHS):\n",
    "        accs = []\n",
    "        gts = []\n",
    "        preds = []\n",
    "        for frames, labels in train_dloader:\n",
    "            frames = frames.to(0)\n",
    "            labels = labels.to(0).float()\n",
    "\n",
    "            pred = model(frames)\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            cat_pred = (pred >= 0.5).float()\n",
    "            acc = (labels == cat_pred).float().mean()\n",
    "            accs.append(acc)\n",
    "            preds.append(cat_pred.detach().cpu().numpy())\n",
    "            gts.append(labels.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        gts = np.concatenate(gts)\n",
    "        preds = np.concatenate(preds)\n",
    "        f1 = f1_score(gts, preds)\n",
    "        acc = sum(accs) / len(accs)\n",
    "        print(f\"(Train) Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "\n",
    "        accs = []\n",
    "        gts = []\n",
    "        preds = []\n",
    "        for frames, labels in test_dloader:\n",
    "            frames = frames.to(0)\n",
    "            labels = labels.to(0).float()\n",
    "\n",
    "            pred = model(frames)\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            cat_pred = (pred >= 0.5).float()\n",
    "            acc = (labels == cat_pred).float().mean()\n",
    "            accs.append(acc)\n",
    "            preds.append(cat_pred.detach().cpu().numpy())\n",
    "            gts.append(labels.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        gts = np.concatenate(gts)\n",
    "        preds = np.concatenate(preds)\n",
    "        f1 = f1_score(gts, preds)\n",
    "        acc = sum(accs) / len(accs)\n",
    "        print(f\"(Test) Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        if f1 > 0.9 and acc > 0.9: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d288f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Train) Acc: 0.6818, F1: 0.6818\n",
      "(Test) Acc: 0.6154, F1: 0.1667\n",
      "(Train) Acc: 0.8864, F1: 0.8571\n",
      "(Test) Acc: 0.6538, F1: 0.4706\n",
      "(Train) Acc: 0.9091, F1: 0.8889\n",
      "(Test) Acc: 0.6923, F1: 0.5556\n",
      "(Train) Acc: 0.9091, F1: 0.8889\n",
      "(Test) Acc: 0.6923, F1: 0.5556\n",
      "(Train) Acc: 0.9545, F1: 0.9474\n",
      "(Test) Acc: 0.6923, F1: 0.5556\n",
      "(Train) Acc: 0.9545, F1: 0.9474\n",
      "(Test) Acc: 0.7692, F1: 0.7000\n",
      "(Train) Acc: 0.9545, F1: 0.9474\n",
      "(Test) Acc: 0.7308, F1: 0.6667\n",
      "(Train) Acc: 0.9318, F1: 0.9189\n",
      "(Test) Acc: 0.8462, F1: 0.8182\n",
      "(Train) Acc: 0.9318, F1: 0.9189\n",
      "(Test) Acc: 0.8077, F1: 0.7619\n",
      "(Train) Acc: 0.9318, F1: 0.9189\n",
      "(Test) Acc: 0.8846, F1: 0.8421\n"
     ]
    }
   ],
   "source": [
    "training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1564019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path in pathlib.Path(\"../output/UR_fall_detection2/\").glob(\"keypoints*\"):\n",
    "    os.rename(str(path), str(path).replace(\".txt\", \".json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd90b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
